{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "using tensorflow you design your own graph and then Tensorflow takes the graph and runs it efficiently using optimized C++ code.Each of the graphs can be broken up into different sections so they can be trained using distribued computing across hundreds of servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.6'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform \n",
    "platform.python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This code does not actually perform any computation\n",
    "# It just creates a computation graph.\n",
    "X = tf.Variable(3, name='X')\n",
    "y = tf.Variable(4, name='y')\n",
    "f = X*X*y + y +2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Session initializes the variables and \n",
    "#  evaluates f.\n",
    "with tf.Session() as sess:\n",
    "    X.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the 'Session' is set as the default session. Calling x.initializer.run() is equivalent to calling tf.get_default_session().run(X.initializer)\n",
    "  \n",
    "This makes the code easier to read and automatically closes it at the end of the block.\n",
    "\n",
    "Instead of manually running the initializer for every single variable, you can use the global_variables_initializer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() #initialize all the variables\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tensorflow program typically split into two parts. The construction and execution phase. The constructino phase typically builds the computation grah representing the ML model and the computations required to train it. \n",
    "\n",
    "The execution phase generallly runs a loop that evaluates a training step repeatedly e.g. one step per mini-batch, gradually improving model parameters.\n",
    "\n",
    "## Managing Graphs\n",
    "any node you create is automatically added to the default graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you may want to manage multiple independent graphs. You can do this by creating a new Graph and temporarily making it the default graph inside a with block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "    \n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifecycle of a Node Value\n",
    "when you evaluate a node, TensorFlow automatically determines the set of nodes that it depends on and it evaluates these nodes first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "X = w + 2\n",
    "y = X + 5\n",
    "z = X * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval()) \n",
    "    print(z.eval()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code when evaluating y and z tensorflow will firt find that y depends on X which depends in W and will evaluate W then X then Y. when evaluating Z in the same session it will repeat this process again and re-evaluate W and X before Z. \n",
    " \n",
    "If you want to evaluate y and Z efficiently. You must ask TensorFlow to evaluate both y and z in just one graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val,z_val = sess.run([y,z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "every session of tensorflow has its on state and does not share with any other. In distributed TesorFlow variable state is stored on servers and not the sessions so multiple sessions can share the same variables.\n",
    "\n",
    "# Linear Regression with Tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.7465141e+01],\n",
       "       [ 4.3573415e-01],\n",
       "       [ 9.3382923e-03],\n",
       "       [-1.0662201e-01],\n",
       "       [ 6.4410698e-01],\n",
       "       [-4.2513184e-06],\n",
       "       [-3.7732250e-03],\n",
       "       [-4.2664889e-01],\n",
       "       [-4.4051403e-01]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "\n",
    "# adds bias to all training instances \n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "\n",
    "\n",
    "# create two tensorflow nodes to hold this data and targets\n",
    "x = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"x\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "\n",
    "\n",
    "# Defines Theta the weights of the linear regression\n",
    "# theta corresponds to the Normal equation\n",
    "XT = tf.transpose(x)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, x)), XT), y)\n",
    "\n",
    "\n",
    "# Starts session to evalluate theta\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "    \n",
    "theta_value # weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "* Random_uniform() creates a node in the graph that will generate a tensor containing random values, given its shape and value range\n",
    "\n",
    "* assign() creates a node that will assign a new value to a variable\n",
    "\n",
    "* The main loop executes the training step n_epochs times and every 1-- iterations prints out the current Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 MSE = 7.589863\n",
      "Epoch: 100 MSE = 4.874135\n",
      "Epoch: 200 MSE = 4.81821\n",
      "Epoch: 300 MSE = 4.8120546\n",
      "Epoch: 400 MSE = 4.80947\n",
      "Epoch: 500 MSE = 4.807726\n",
      "Epoch: 600 MSE = 4.8064837\n",
      "Epoch: 700 MSE = 4.805584\n",
      "Epoch: 800 MSE = 4.8049355\n",
      "Epoch: 900 MSE = 4.8044643\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() \n",
    "tf.set_random_seed(42)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Scale the inputs \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "scaled_housing_data_plus_bias = scaler.fit_transform(housing_data_plus_bias)\n",
    "\n",
    "\n",
    "# Create two constant tensors for inputs. \n",
    "# like multidimensional arrays\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")\n",
    "\n",
    "# Creates a Variable\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "\n",
    "# predictions are input data X multiplied by the weights\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "\n",
    "#Calculate errors\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "# differential of the cost function.\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "\n",
    "# Reassigning the new weights.\n",
    "trainin_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # declaring the variables.\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch:\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(trainin_op) #automatically evaluates all the dependencies\n",
    "    best_tehta = theta.eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autodiff \n",
    "there are efficient and non efficient ways of determining the partial derivatiives aka gradients. The above mathematical method for determining gradients is not always the most efficient. Tensorflow have their own autodiff method to use the best version.   \n",
    "\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "\n",
    "the gradients function takes an operation in this case mean squared error and a list of variables in this case just theta and creates a list of ops one per variable to compute the gradients of the op with regard to the variable. So the gradients node will compute the gradient vector of the MSE with regards to theta.\n",
    "\n",
    "### Using an Optimizer \n",
    "tensor flow can even provide out of the box optimizers for you e.g. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Data to the Training Algorithm\n",
    "To implement Mini-batch Gradient Descent we need to replace X and y at every iteration within the next mini-batch. Place holder node will help. they dont perform any computation, just output the data you tell them to output at runtime. They are typically used to pass the training data to Tensorflow at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 7. 8.]]\n",
      "[[ 9. 10. 11.]\n",
      " [12. 13. 14.]]\n"
     ]
    }
   ],
   "source": [
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A:[[4,5,6],[7,8,9]]})\n",
    "    \n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n+1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "batch_size = 100\n",
    "# m being the \n",
    "n_batches = int(np.ceil(m / batch_size)) \n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    [...] #load the data from disk\n",
    "    return X_batch, y_batch\n",
    "with tf.session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
