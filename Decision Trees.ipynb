{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Visualizing a Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, 2:]\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree will look something like this. \n",
    "<br/>\n",
    "<img src=\"images/initial_tree.png\">\n",
    "<br/>\n",
    "Decision tree's require very little data preperation they are not sensitive to scaling at all. \n",
    "* \"sample\" attribute counts how many training instances a node applies to. \n",
    "* \"value\" attribute tells you how many training instances of each class this node applies to.\n",
    "* \"gini\" attribute measures a nodes impurity. if a node is pure (gini=0) all training instances it applies to belong to the same class. \n",
    "\n",
    "<br/>\n",
    "The Gini imputrity is calculated as follows \n",
    "<br/>\n",
    "<img src=\"images/gini_eq.png\">\n",
    "<br/>  \n",
    "\n",
    "# Estimating Class Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART training \n",
    "Sci-kit learn uses the Classification and regression tree (CART algorithm to train trees. It's quite simple firs the algorithm splits the training instances into two subsets using a single feature and threshold. the algorithm searches the feature and threshold that produces the lowest impurity. The cost function is below. \n",
    "<br/>\n",
    "<img src=\"images/CART.png\">\n",
    "<br/>\n",
    "The algorithm will keep splitting nodes until either it reaches the specified maximum depth specified in sklearn by \"max_depth\" or it cannot split the node in anyway that will improve it's impurity.\n",
    "\n",
    "### Computational Complexity\n",
    "The complexity is O(n * Mlog(m)) for small training sets you can presort the data by setting presort=True however this considerable slows down training for much larger training sets.\n",
    "\n",
    "### Regularization Hyperparameters\n",
    "training without any reularization will cause overfitting. in linear Regression assumptions are made about the data before training begins. e.g. it is a linear trend. without regularization the tree will fit training data very closely. \n",
    "\n",
    "* \"max_dpeth\" restricts the maximum depth of the Decision Tree\n",
    "* \"min_samples_split\" the minimum number of samples a node must have before it can split.\n",
    "* \"min_samples_leaf\" the minimum number of samples a leaf can have\n",
    "* \"max_leaf_nodes\" maximum number of leaf nodes.\n",
    "\n",
    "## Tree Regression\n",
    "\n",
    "                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.09259259])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg.predict([[5, 1.5]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree looks very similar to the classification above but instead of prediciting a class it predicts a value. This value is the average target value for all of the instances of the leaf node. The cost function looks very similar but instead of using the impurity level it uses MSE. \n",
    "<br/>\n",
    "<img src=\"images/CART_reg.png\">\n",
    "<br/>\n",
    "again without regularzation the models are prone to overfitting.   \n",
    "  \n",
    "## Instability  \n",
    "Decision Trees have alot going for them. they are easy to understand interpret and require little data preperation. There decision boundary's however happen to be orthogonal which means they can be sensitive to the rotation of datasets. It is advantageous to use PCA which often results in a better orientation fo the training data. The training algorithm for the Tree's in sci-kit learn is stochasic so can produce different results for the same training data. a solution to this issue may be to use ensemble's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
