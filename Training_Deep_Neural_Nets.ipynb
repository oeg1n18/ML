{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep neural nets \n",
    "\n",
    "should you want to solve a more complex problem of identifying images in high resolution images. It would be necessary to include possible 10 layers each contaiing hundreds of neurons with 100,000s of connections. The issues that would arise in training \n",
    "\n",
    "* Vanashing/exploding Gradients problem can make lower layers hard to train \n",
    "* models with large network are extrememly slow to train\n",
    "* A module with millions of parameters would severely risk overfittin ghte training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients Problems \n",
    "in backpropagation when calculating the gradients they get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually untrained and never converges to a solution. \n",
    "\n",
    "In some cases the gradients of the lower levels can grow bigger and bigger, the deep neural networks suffer from unstable gradients. \n",
    "\n",
    "This problem is increased using a random initialisation of gaussian distribution and a sigmoid function as it has a mean of 0.5. The varience keeps increasing after each layer. \n",
    "\n",
    "For high and lower values there is virtually no gradient on the sigmoid function. This reduces the effectiveness of gradient descent to converge. What gradient exists keeps gettin diluted as backrpropagation progresses through the top lay3rs sos there si really nothing left for the lower levels. \n",
    "\n",
    "### Zavier and He initialisation \n",
    "They showed that to solve the problem the varience of both the inputs and outputs of a layer in both directions of backpropogation need to have the same varience which is not possible to occur unless the layers have equal number of inputs and outputs. \n",
    "\n",
    "They proposed a solution that works very well in practice using the following initialisation scheme. \n",
    "\n",
    "normal distribution with mean 0 and standard deviation \n",
    "$\\sigma = \\sqrt{\\frac{2}{n_{intputs} + n_{output}}}$\n",
    "\n",
    "or a unifom distribution between -r and +r with \n",
    "$ r = \\sqrt{\\frac{6}{n_{intputs} + n_{output}}}$\n",
    "\n",
    "in pactice this looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, \n",
    "                          kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions \n",
    "The Sigmoid saturates at high and low values. This Relu function is very fast to comput however suffers from dying. e.g. once the weights are altered to receve a 0. they no longer have a nonzero derivative and become effectively useless. \n",
    "\n",
    "A solution is the use of a leaky relu function LeakyReLU = $max(\\alpha z, z)$ the hyperparamemter $\\apha$ defines how much the function leaks. So it can still die but has a higher chance of coming to life. $\\alpha$ can be picked randomly in a given range during training, and it is fixed to an average value during testing. It acts as a regulization techique reducing overfitting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
