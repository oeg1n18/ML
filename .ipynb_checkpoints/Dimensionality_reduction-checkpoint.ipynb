{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "big datasets can have millions of features. Some of these features may be unimportant. for example the pixels around the edge of an instance of the MNIST dataset. others for example may be highly correlated and redcing them together will not loose alot of information. \n",
    "\n",
    "Also dimensionality reducting can be very useful reducing a number of features into mabye just 3 dimensions which can then be graphed.\n",
    "\n",
    "***The three popular dimensions are PCA, kernel and LLE.***\n",
    "\n",
    "if you picked two points in a unit square the average distance would be roughly 0.52 bit in 1,000,000 dimensions would be around 408.25 which means that it is unlikely any other instance will be replicas and the datasets are at risk of being very sparse meaning all the instances will be very far apart making predictions less reliable than in lower dimensions. In essesnce the more features you have the more prone your model will be to overfitting. Therefore Dimensionality reduction can imporve performance.\n",
    "\n",
    "##  Projection\n",
    "in a 3d dataset if you simply project the datapoints onto a single plane you have reduced 3d to 2d however if there are distinct layers in the 3d dataset projecting them ontop of eachother will lose valuable information. \n",
    "\n",
    "## Manifold learning\n",
    "Manifold learning put simply is a 3d shape rolled out onto a 2d plane. a d-dimensional manifold is part of an n-dimensional space where d<n. The manifold hypothesis that many high dimensional datasets resemble closely to a lower dimensional manifold. \n",
    "\n",
    "Not all manifolds are simpler than their original spaces. Their decision boundaries may be more complex and therefore it is not always the case that maifold learning will produce a better training set. It depends on the dataset.\n",
    "\n",
    "## Principle Component Analysis\n",
    "it is by far the best dimensionality reduction algorithm. It first identifies the hyperplane that lies closest to the data and then projects the data onto it. \n",
    "\n",
    "#### Preserving Varience\n",
    "depending on the hyperplane you use for projection varience will be reduced in some form. This removes some of the important information from the dataset. A dimension must be chosen so that the least amount of varience will be lost. for example if we reduce from 3 to 2 dimensions the dimensions chosen would be the ones that retained the most varience.\n",
    "\n",
    "Single Value Decomposiition is a matrix factorization method that decomposes the training set. Numpy has a method to find the principle componenets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "train_set = (load_digits())['data']\n",
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_centered = train_set - train_set.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:,0] #all dimension values for instance 1\n",
    "c1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2 = Vt.T[:,1] #all dimension values for instance 2 \n",
    "c2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#following code projects the traininset onto the plane defined\n",
    "# by the first two principle compoenents\n",
    "\n",
    "W2 = Vt.T[:,:2]\n",
    "X2D = X_centered.dot(W2) #this is how you reduce the dimensionality \n",
    "# fo any dataset.\n",
    "X2D.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Sklearns PCA class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 10)\n",
    "X2D = pca.fit_transform(X_centered)\n",
    "X2D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explained Varience Ratio\n",
    "below shows the level varience that is present in each of the principle componenets in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14890594, 0.13618771, 0.11794594, 0.08409979, 0.05782411,\n",
       "       0.04916909, 0.04315969, 0.03661322, 0.03353245, 0.030784  ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Choosing the right nubmer of Dimensions***\n",
    "the general method is to reduce the dimensions to a level that only reduces the total varience to about 0.95. s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit((load_digits())['data'])\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 29)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or \n",
    "pca = PCA(n_components=0.95)\n",
    "data_reduced = pca.fit_transform((load_digits())['data'])\n",
    "data_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA for Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x132e553d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAALGElEQVR4nO3df6jddR3H8dfLu+lS58+sdHe4BbYQKSeXiayMNqyZov4RsYVSEgwMx6RAtH8iCPpPlKjBmDPL5cipIGIz8Ucm1XS/Mre7yVrG7nI/1MbmwF2ve/fHPYMpV+/3nPv9dd88HzC8957D+byP+tz33O899/txRAhAHqc0PQCAchE1kAxRA8kQNZAMUQPJTKniQU/1aTFNZ1Tx0I0anlHvc7rs/IO1rfXO8b7a1np7Z33/HuP9kdrWqtN7OqrhOOaxbqsk6mk6Q1d6YRUP3ah/L7uq1vVe/t6K2tZae+Tc2tb63dfm1bbWyL79ta1Vpw3x7MfexstvIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpIhaiCZQlHbXmR7p+1dtu+qeigAvRs3att9kn4l6VpJl0paYvvSqgcD0JsiR+p5knZFxO6IGJa0VtKN1Y4FoFdFop4hac9Jnw91vvYhtpfa3mh74/s6VtZ8ALpU2omyiFgZEQMRMTBVp5X1sAC6VCTqvZJmnvR5f+drAFqoSNSvSLrE9mzbp0paLOmJascC0KtxL5IQESO2b5f0tKQ+SasjYlvlkwHoSaErn0TEU5KeqngWACXgHWVAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMpXs0FGn11fUt9vDLxasrW0tSbrsvh/WttZry39d21q//Oqs2tY685GcO3R8Eo7UQDJEDSRD1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kU2SHjtW2D9h+rY6BAExMkSP1byQtqngOACUZN+qIeFHSOzXMAqAEpf2Wlu2lkpZK0jSdXtbDAugS2+4AyXD2G0iGqIFkivxI62FJf5M0x/aQ7R9UPxaAXhXZS2tJHYMAKAcvv4FkiBpIhqiBZIgaSIaogWSIGkiGqIFkHBGlP+hZPi+u9MLSH3csp3zpi7WsI0mnHPhfbWtJ0i1/frnW9erywJyLmx5h0tsQz+pwvOOxbuNIDSRD1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMkWuUTbT9vO2t9veZnt5HYMB6E2Ri/mPSPpxRGy2PV3SJtvPRMT2imcD0IMi2+68GRGbOx8fkTQoaUbVgwHoTVfb7tieJWmupA1j3Ma2O0ALFD5RZvtMSY9KuiMiDn/0drbdAdqhUNS2p2o06DUR8Vi1IwGYiCJnvy3pfkmDEXFP9SMBmIgiR+r5km6RtMD21s6fb1U8F4AeFdl25yVJY142BUD78I4yIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpLp6re02uj4qzvqW6zGfbskafH0+vbu+s7uevY+k6Qpn6vvf7uRfftrW6stOFIDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8kUufDgNNsv2/5HZ9udn9UxGIDeFHm/3jFJCyLi3c6lgl+y/ceI+HvFswHoQZELD4akdzufTu38iSqHAtC7ohfz77O9VdIBSc9ExJjb7tjeaHvj+zpW8pgAiioUdUR8EBGXS+qXNM/2ZWPch213gBbo6ux3RByS9LykRZVMA2DCipz9vsD2OZ2PPyXpGkk1/hIzgG4UOft9oaQHbfdp9C+BP0TEk9WOBaBXRc5+v6rRPakBTAK8owxIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZCb9tjt1qnWLH0nXXfHN2taau/6/ta2l9fUttWXRRfUtpnZs88ORGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZApH3bmg/xbbXHQQaLFujtTLJQ1WNQiAchTddqdf0nWSVlU7DoCJKnqkvlfSnZKOf9wd2EsLaIciO3RcL+lARGz6pPuxlxbQDkWO1PMl3WD7DUlrJS2w/VClUwHo2bhRR8TdEdEfEbMkLZb0XETcXPlkAHrCz6mBZLq6nFFEvCDphUomAVAKjtRAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMmy702J1buFS5/Y0b6+eXtta+396Xm1rSdIXbmPbHQAlI2ogGaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIJlCbxPtXEn0iKQPJI1ExECVQwHoXTfv/f56RLxV2SQASsHLbyCZolGHpD/Z3mR76Vh3YNsdoB2Kvvz+SkTstf0ZSc/Y3hERL558h4hYKWmlJJ3l86LkOQEUVOhIHRF7O/88IOlxSfOqHApA74pskHeG7eknPpb0DUmvVT0YgN4Uefn9WUmP2z5x/99HxPpKpwLQs3Gjjojdkr5cwywASsCPtIBkiBpIhqiBZIgaSIaogWSIGkiGqIFk2HanC6+vqPfdsRc959rWeu/c+v5+/+2l99S21k2HbqttrbbgSA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJEPUQDKForZ9ju11tnfYHrR9VdWDAehN0fd+3ydpfUR82/apkk6vcCYAEzBu1LbPlnS1pO9LUkQMSxqudiwAvSry8nu2pIOSHrC9xfaqzvW/P4Rtd4B2KBL1FElXSFoREXMlHZV010fvFBErI2IgIgam6rSSxwRQVJGohyQNRcSGzufrNBo5gBYaN+qI2Cdpj+05nS8tlLS90qkA9Kzo2e9lktZ0znzvlnRrdSMBmIhCUUfEVkkD1Y4CoAy8owxIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZNhLqwtTD/XVut6yn6+tdb263PTX+va3+vx3t9a2VltwpAaSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkhk3attzbG896c9h23fUMBuAHoz7NtGI2Cnpckmy3Sdpr6THqx0LQK+6ffm9UNK/IuI/VQwDYOK6/YWOxZIeHusG20slLZWkaeyfBzSm8JG6c83vGyQ9MtbtbLsDtEM3L7+vlbQ5IvZXNQyAiesm6iX6mJfeANqjUNSdrWuvkfRYteMAmKii2+4clXR+xbMAKAHvKAOSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogGUdE+Q9qH5TU7a9nflrSW6UP0w5ZnxvPqzkXR8QFY91QSdS9sL0xIgaanqMKWZ8bz6udePkNJEPUQDJtinpl0wNUKOtz43m1UGu+pwZQjjYdqQGUgKiBZFoRte1Ftnfa3mX7rqbnKYPtmbaft73d9jbby5ueqUy2+2xvsf1k07OUyfY5ttfZ3mF70PZVTc/Urca/p+5sEPC6Ri+XNCTpFUlLImJ7o4NNkO0LJV0YEZttT5e0SdJNk/15nWD7R5IGJJ0VEdc3PU9ZbD8o6S8RsapzBd3TI+JQw2N1pQ1H6nmSdkXE7ogYlrRW0o0NzzRhEfFmRGzufHxE0qCkGc1OVQ7b/ZKuk7Sq6VnKZPtsSVdLul+SImJ4sgUttSPqGZL2nPT5kJL8z3+C7VmS5kra0PAoZblX0p2Sjjc8R9lmSzoo6YHOtxarOhfdnFTaEHVqts+U9KikOyLicNPzTJTt6yUdiIhNTc9SgSmSrpC0IiLmSjoqadKd42lD1HslzTzp8/7O1yY921M1GvSaiMhyeeX5km6w/YZGv1VaYPuhZkcqzZCkoYg48YpqnUYjn1TaEPUrki6xPbtzYmKxpCcanmnCbFuj35sNRsQ9Tc9Tloi4OyL6I2KWRv9bPRcRNzc8VikiYp+kPbbndL60UNKkO7HZ7QZ5pYuIEdu3S3paUp+k1RGxreGxyjBf0i2S/ml7a+drP4mIp5obCQUsk7Smc4DZLenWhufpWuM/0gJQrja8/AZQIqIGkiFqIBmiBpIhaiAZogaSIWogmf8Dh++pru7E5f8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "digits = (load_digits())['data']\n",
    "plt.imshow(digits[3].reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets compress it \n",
    "pca = PCA(n_components = 0.95)\n",
    "im_reduced = pca.fit_transform(digits)\n",
    "im_reduced[2].shape #down to only 29 components fro 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x132be5bb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im_reconstructed = pca.inverse_transform(im_reduced)\n",
    "plt.imshow(im_reconstructed[3].reshape(8,8)) \n",
    "# not bad pretty much the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue with the PCA method is that it requires the entire dataset to fit into memory. For large Datasets where this is not possible you must use IPCA algorithm which allows mini-batches for on the fly taining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:  (1797, 64)\n",
      "reduced:  (1797, 29)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "\n",
    "n_batches = 60\n",
    "inc_pca = IncrementalPCA(n_components=29)\n",
    "for X_batch in np.array_split(digits, n_batches): \n",
    "    inc_pca.partial_fit(X_batch)\n",
    "    \n",
    "im_reduced = inc_pca.transform(digits)\n",
    "\n",
    "print('original: ', digits.shape)\n",
    "print('reduced: ', im_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA\n",
    "we saw in using SVM the rbf kernel method allowed he mapping of a dataset into a higher dimension space (feature space) enabiling nonlinear classification and regression with SVMs. \n",
    "\n",
    "remember a linear seperatable datsets in the feature space or complex nonlinear in the original space. The same kernel method can perform complex nonlinear projections for dimensionality reduction.It is good at preserving clusters of instance after projections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components=3, kernel='rbf', gamma=0.4)\n",
    "dig_reduced = rbf_pca.fit_transform(digits)\n",
    "dig_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting a Kernel and Tuning Hyperparameters\n",
    "the kPCA (kernel pca) method is an unsupervised learnin algorithm. There is no obvious performance measure to help you select the best hyperparameter but you could use a grid search to find the best solution and then include that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "y = t > 6.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('kpca', KernelPCA(n_components=2)),\n",
       "                                       ('log_reg', LogisticRegression())]),\n",
       "             param_grid=[{'kpca__gamma': array([0.03      , 0.03222222, 0.03444444, 0.03666667, 0.03888889,\n",
       "       0.04111111, 0.04333333, 0.04555556, 0.04777778, 0.05      ]),\n",
       "                          'kpca__kernel': ['rbf', 'sigmoid']}])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"kpca\", KernelPCA(n_components=2)),\n",
    "    (\"log_reg\", LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = [{\"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "               \"kpca__kernel\": [\"rbf\", \"sigmoid\"]}]\n",
    "\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_ #the best parameters to use for kPCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead of finding the kernel that produces the best accuracy for the model another method would to find the best hyperparameters that produces the least errors for reconstruction however when you use a kernel method to map a training set into a smaller dimension you effectively first map the training set into a infinate dimensional space and then use PCA to reduce it. It is not therefore possible to be able to map points between sets and not possible to find the error. \n",
    "\n",
    "A solution is to find the point in the reduced set that is closest to one in the training set. As they have a relatively large number of dimensions all points are effectively very far away so the close ones are very likely to be the same points in different space. You can then yield the mean squared error and quantify the reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.433, fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)\n",
    "# by defuatl kernelPCA has fit_inverse_transofrm as false so has \n",
    "# no inverse_transform method unless set true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.81663597867971e-26"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute the pre-image reconstruction error \n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(X, X_preimage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797, 64)\n",
      "(1797, 64) (1797, 64)\n",
      "(1797, 64) (1797, 64)\n",
      "(1797, 64) (1797, 64)\n",
      "(1797, 64) (1797, 64)\n",
      "(1797, 64) (1797, 64)\n"
     ]
    }
   ],
   "source": [
    "# lets use gridsearch to find the hyperparameters that reduce \n",
    "# the reconstruction error the most.\n",
    "\n",
    "models = []\n",
    "param_grid = [{\"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "               \"kpca__kernel\": [\"rbf\", \"sigmoid\"]}]\n",
    "\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "y = t > 6.9\n",
    "\n",
    "X = load_digits()['data']\n",
    "\n",
    "for model_kernel in [\"rbf\", \"sigmoid\"]:\n",
    "    for model_gamma in (0.03, 0.05, 10):\n",
    "        models.append(KernelPCA(n_components=12, kernel=model_kernel,\n",
    "                               gamma=model_gamma, fit_inverse_transform=True))\n",
    "\n",
    "model_errors = {}\n",
    "for model in models: \n",
    "    X_reduced = model.fit_transform(X)\n",
    "    X_preimage = model.inverse_transform(X_reduced)\n",
    "    print(X.shape, X_preimage.shape)\n",
    "    model_errors[mean_squared_error(X, X_preimage)] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPCA(fit_inverse_transform=True, gamma=10, kernel='sigmoid',\n",
       "          n_components=12)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for key in model_errors.keys():\n",
    "    mini = np.inf\n",
    "    val = key\n",
    "    if val<mini: \n",
    "        mini = val\n",
    "        \n",
    "best_model = model_errors[mini]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.538229973096401e-25"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini # better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLE\n",
    "Locally Linear Embedding. It is a Manifold Learning technique that does not rely on projections like previous algorithms. \n",
    "\n",
    "1. LLE first finds the K-nearest neighbors and then attempts to represent the instance as a linear combination of its neighbors. \n",
    "\n",
    "2. do a weighted aggregation of the neighbours to minimize the cost function, where j'th nearest neighbour for point Xi\n",
    "<br/>\n",
    "<img src=\"images/LLE_cost.png\")\n",
    "<br/>\n",
    " \n",
    "3. Now that we have the weights ***W*** that minimized the cost function above and encodes the local linear relationships between instances. The next step is to map the instances into a d-dimensional space d<n while preserving local relationships.\n",
    "\n",
    "4. next we minimize the folowing. \n",
    "<img src=\"images/LLE_step_two.png\">\n",
    "\n",
    "where ***Z*** is the image of ***X*** in d dimensions. It looks similar to the first optimization but instead of keeping the instances fixing and findng the optimal weights. Keep the weights fixed and find the optimal position of the instances int he lower dimensional space.\n",
    "\n",
    "5. LLE has the complexity O(m log(m)n log(k)) for finding k nearest neighbors, O(mnk^3) for optimizing weights and O(dm^2) for constructing low-dimensional representations. The m^2 in the last term makes the alogrithm scale poorly to large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
