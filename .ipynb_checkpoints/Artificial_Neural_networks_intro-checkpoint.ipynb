{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial_Neural_networks_intro\n",
    "  \n",
    "    \n",
    "### Logical Computations with Neurons\n",
    "Warren McCulloch and Walter Pitts proposed a very simple network with which neurons which could have identical properties to logic gates. Complex models can be built from this just like hardware.\n",
    "\n",
    "### The Perceptron\n",
    "invented by Frank Rosenblatt. It is a single layer network using linear threshold units. The outputs are numbers instead of binary input/output values like the neruron above. The LTU computes a weighted sum of its inputs and then applies a step function to that sum and outputs the result. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    h_w = step(z) = step(w^TX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perceptron Learning Rules:\n",
    "w_i,j(next_step) = w_i,j + n(yJ - yhat_j)x_i\n",
    "\n",
    "* W_i,j is the connection weight between the i^th input neuron and the j^th output neuron.\n",
    "* y_hat is the output of the j^th output neuron for the current training instance. \n",
    "* y_j is the target output of the j^th output neuron for the current training instance. \n",
    "* n is the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2,3)] #petal length, petal width\n",
    "y = (iris.target ==0).astype(np.int) # Iris Setosa\n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "\n",
    "##### Perceptrons cannot make class prediction probabilities only classifications #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MLP (multi layer perceptron) consists of one or more layers of TLUs (hidden layers) and one final layer of TLUs called the output layer. \n",
    "\n",
    "to train using reverse-mode autodiff. The error is measure between the networks output error and then it computes how much of the error was contributed to by each neuron in the hidden layer. This pass efficiently measures the error gradient across all the connection weights in the netwok by propagating the error gradient backward in the network.\n",
    "\n",
    "in order for this algorithm to work logistic function should be used instead of step for the perceptrons. \n",
    "1/ (1+exp(-z)). It is differentiable and output value ranges from -1 to 1 so gradients can be calculated and output is more normalized.\n",
    "\n",
    "### Training a DNN using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28*28 # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") #To feed batches to during training\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name): #name scope using name of layer\n",
    "        n_inputs = int(X.get_shape()[1]) #get the number of inputs\n",
    "        stddev = 2 / np.sqrt(n_inputs + n_neurons) #standard deviation of distribution\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)#random values from a truncated normal dist\n",
    "        W = tf.Variable(init, name=\"kernel\") #weights\n",
    "        b = tf.Variable(tf.zeros([n_neruons]), name=\"bias\") #bias\n",
    "        Z = tf.matmul(X, W)+b #prediction\n",
    "        if activation is not None: \n",
    "            return activation(X)\n",
    "        else: \n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-1bbe3ab56db0>:3: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /Users/olivergrainge/opt/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "#Creating layers\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "    # cost funcition\n",
    "    # xentropy is equivalent to applying the softmax activation function\n",
    "    # and then computing cross entropy.\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "    # Training using GradientDescent\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss) #minimizing loss function\n",
    "    \n",
    "    #using accuracy as a performance measure\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "0 Train accuracy: 0.86 Val accuracy; 0.906\n",
      "1 Train accuracy: 0.98 Val accuracy; 0.9252\n",
      "2 Train accuracy: 0.9 Val accuracy; 0.9338\n",
      "3 Train accuracy: 0.96 Val accuracy; 0.94\n",
      "4 Train accuracy: 0.9 Val accuracy; 0.9462\n",
      "5 Train accuracy: 0.96 Val accuracy; 0.95\n",
      "6 Train accuracy: 0.96 Val accuracy; 0.9524\n",
      "7 Train accuracy: 0.98 Val accuracy; 0.9574\n",
      "8 Train accuracy: 0.96 Val accuracy; 0.9602\n",
      "9 Train accuracy: 0.98 Val accuracy; 0.9612\n",
      "10 Train accuracy: 0.98 Val accuracy; 0.9634\n",
      "11 Train accuracy: 0.96 Val accuracy; 0.9666\n",
      "12 Train accuracy: 0.98 Val accuracy; 0.9678\n",
      "13 Train accuracy: 0.98 Val accuracy; 0.9682\n",
      "14 Train accuracy: 0.94 Val accuracy; 0.9702\n",
      "15 Train accuracy: 0.94 Val accuracy; 0.9712\n",
      "16 Train accuracy: 1.0 Val accuracy; 0.9712\n",
      "17 Train accuracy: 1.0 Val accuracy; 0.9726\n",
      "18 Train accuracy: 0.98 Val accuracy; 0.9724\n",
      "19 Train accuracy: 1.0 Val accuracy; 0.9746\n",
      "20 Train accuracy: 0.98 Val accuracy; 0.9734\n",
      "21 Train accuracy: 0.96 Val accuracy; 0.9748\n",
      "22 Train accuracy: 0.96 Val accuracy; 0.9762\n",
      "23 Train accuracy: 1.0 Val accuracy; 0.9764\n",
      "24 Train accuracy: 0.98 Val accuracy; 0.9768\n",
      "25 Train accuracy: 1.0 Val accuracy; 0.9778\n",
      "26 Train accuracy: 0.98 Val accuracy; 0.977\n",
      "27 Train accuracy: 0.98 Val accuracy; 0.9786\n",
      "28 Train accuracy: 1.0 Val accuracy; 0.9782\n",
      "29 Train accuracy: 1.0 Val accuracy; 0.9782\n",
      "30 Train accuracy: 0.98 Val accuracy; 0.979\n",
      "31 Train accuracy: 0.96 Val accuracy; 0.9756\n",
      "32 Train accuracy: 1.0 Val accuracy; 0.9796\n",
      "33 Train accuracy: 1.0 Val accuracy; 0.979\n",
      "34 Train accuracy: 1.0 Val accuracy; 0.979\n",
      "35 Train accuracy: 1.0 Val accuracy; 0.9802\n",
      "36 Train accuracy: 0.98 Val accuracy; 0.9782\n",
      "37 Train accuracy: 1.0 Val accuracy; 0.9808\n",
      "38 Train accuracy: 1.0 Val accuracy; 0.9812\n",
      "39 Train accuracy: 1.0 Val accuracy; 0.981\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X:mnist.validation.images, y: mnist.validation.labels})\n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy;\", acc_val)\n",
    "        \n",
    "    save_path = saver.save(sess, \"models/tensorflow/my_model_final.cpkt:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
