{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial_Neural_networks_intro\n",
    "  \n",
    "    \n",
    "### Logical Computations with Neurons\n",
    "Warren McCulloch and Walter Pitts proposed a very simple network with which neurons which could have identical properties to logic gates. Complex models can be built from this just like hardware.\n",
    "\n",
    "### The Perceptron\n",
    "invented by Frank Rosenblatt. It is a single layer network using linear threshold units. The outputs are numbers instead of binary input/output values like the neruron above. The LTU computes a weighted sum of its inputs and then applies a step function to that sum and outputs the result. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    h_w = step(z) = step(w^TX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perceptron Learning Rules:\n",
    "w_i,j(next_step) = w_i,j + n(yJ - yhat_j)x_i\n",
    "\n",
    "* W_i,j is the connection weight between the i^th input neuron and the j^th output neuron.\n",
    "* y_hat is the output of the j^th output neuron for the current training instance. \n",
    "* y_j is the target output of the j^th output neuron for the current training instance. \n",
    "* n is the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2,3)] #petal length, petal width\n",
    "y = (iris.target ==0).astype(np.int) # Iris Setosa\n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "\n",
    "##### Perceptrons cannot make class prediction probabilities only classifications #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An MLP (multi layer perceptron) consists of one or more layers of TLUs (hidden layers) and one final layer of TLUs called the output layer. \n",
    "\n",
    "to train using reverse-mode autodiff. The error is measure between the networks output error and then it computes how much of the error was contributed to by each neuron in the hidden layer. This pass efficiently measures the error gradient across all the connection weights in the netwok by propagating the error gradient backward in the network.\n",
    "\n",
    "in order for this algorithm to work logistic function should be used instead of step for the perceptrons. \n",
    "1/ (1+exp(-z)). It is differentiable and output value ranges from -1 to 1 so gradients can be calculated and output is more normalized.\n",
    "\n",
    "### Training a DNN using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_inputs = 28*28 # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") #To feed batches to during training\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name): #name scope using name of layer\n",
    "        n_inputs = int(X.get_shape()[1]) #get the number of inputs\n",
    "        stddev = 2 / np.sqrt(n_inputs + n_neurons) #standard deviation of distribution\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)#random values from a truncated normal dist\n",
    "        W = tf.Variable(init, name=\"kernel\") #weights\n",
    "        b = tf.Variable(tf.zeros([n_neruons]), name=\"bias\") #bias\n",
    "        Z = tf.matmul(X, W)+b #prediction\n",
    "        if activation is not None: \n",
    "            return activation(X)\n",
    "        else: \n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating layers\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "    # cost funcition\n",
    "    # xentropy is equivalent to applying the softmax activation function\n",
    "    # and then computing cross entropy.\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "    # Training using GradientDescent\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss) #minimizing loss function\n",
    "    \n",
    "    #using accuracy as a performance measure\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-b3a0e102ffcc>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/olivergrainge/opt/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/olivergrainge/opt/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /Users/olivergrainge/opt/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /Users/olivergrainge/opt/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/olivergrainge/opt/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "0 Train accuracy: 0.88 Val accuracy; 0.9042\n",
      "1 Train accuracy: 0.98 Val accuracy; 0.9236\n",
      "2 Train accuracy: 1.0 Val accuracy; 0.9332\n",
      "3 Train accuracy: 0.96 Val accuracy; 0.9402\n",
      "4 Train accuracy: 0.92 Val accuracy; 0.947\n",
      "5 Train accuracy: 0.94 Val accuracy; 0.952\n",
      "6 Train accuracy: 0.96 Val accuracy; 0.9554\n",
      "7 Train accuracy: 1.0 Val accuracy; 0.9594\n",
      "8 Train accuracy: 0.94 Val accuracy; 0.9606\n",
      "9 Train accuracy: 1.0 Val accuracy; 0.9626\n",
      "10 Train accuracy: 0.98 Val accuracy; 0.965\n",
      "11 Train accuracy: 1.0 Val accuracy; 0.9668\n",
      "12 Train accuracy: 1.0 Val accuracy; 0.968\n",
      "13 Train accuracy: 0.96 Val accuracy; 0.9704\n",
      "14 Train accuracy: 1.0 Val accuracy; 0.9706\n",
      "15 Train accuracy: 1.0 Val accuracy; 0.9716\n",
      "16 Train accuracy: 0.96 Val accuracy; 0.9716\n",
      "17 Train accuracy: 1.0 Val accuracy; 0.9704\n",
      "18 Train accuracy: 0.96 Val accuracy; 0.972\n",
      "19 Train accuracy: 0.98 Val accuracy; 0.9722\n",
      "20 Train accuracy: 0.96 Val accuracy; 0.9724\n",
      "21 Train accuracy: 0.98 Val accuracy; 0.9738\n",
      "22 Train accuracy: 1.0 Val accuracy; 0.974\n",
      "23 Train accuracy: 0.98 Val accuracy; 0.974\n",
      "24 Train accuracy: 0.98 Val accuracy; 0.9752\n",
      "25 Train accuracy: 0.98 Val accuracy; 0.9752\n",
      "26 Train accuracy: 0.96 Val accuracy; 0.975\n",
      "27 Train accuracy: 0.96 Val accuracy; 0.9758\n",
      "28 Train accuracy: 0.98 Val accuracy; 0.975\n",
      "29 Train accuracy: 0.96 Val accuracy; 0.9744\n",
      "30 Train accuracy: 1.0 Val accuracy; 0.9762\n",
      "31 Train accuracy: 1.0 Val accuracy; 0.976\n",
      "32 Train accuracy: 0.98 Val accuracy; 0.9778\n",
      "33 Train accuracy: 0.98 Val accuracy; 0.9764\n",
      "34 Train accuracy: 0.98 Val accuracy; 0.9756\n",
      "35 Train accuracy: 1.0 Val accuracy; 0.9778\n",
      "36 Train accuracy: 0.98 Val accuracy; 0.9776\n",
      "37 Train accuracy: 0.98 Val accuracy; 0.978\n",
      "38 Train accuracy: 0.96 Val accuracy; 0.9764\n",
      "39 Train accuracy: 1.0 Val accuracy; 0.9784\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch, y:y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X:mnist.validation.images, y: mnist.validation.labels})\n",
    "        \n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy;\", acc_val)\n",
    "        \n",
    "    save_path = saver.save(sess, \"models/tensorflow/my_model_final.cpkt:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restroing and using the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/tensorflow/my_model_final.cpkt:\n",
      "predicted classes: [9 5 1 2 2 1 9 0 9 7 5 8 8 1 5]\n",
      "actual classes:  [9 5 1 2 2 1 9 0 9 7 5 8 8 1 5]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: \n",
    "    saver.restore(sess, \"models/tensorflow/my_model_final.cpkt:\")\n",
    "    X_new_scaled = X_batch[:15]\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1)\n",
    "    \n",
    "print(\"predicted classes:\", y_pred)\n",
    "print(\"actual classes: \", y_batch[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Hidden layers \n",
    "\n",
    "It is possible to model even the most complex functions with just one neuron layer. However MLP have a much higher parameter efficiency. Deeper layers model the basic characteristics of the dataset and higher levels model the finer detail. The deeper layers can therefore be utilised for different purposes reducing training times. \n",
    "\n",
    "The DNN also converge faster as the lower layers model the more basic characteristics and higher layer the finer ones. \n",
    "\n",
    "## Number of Neurons per HIdden Layer\n",
    "as a rule of thumb you will get better accuracy increasing the number of layers than the number of neurons per layer. provided all of the layers have the same number of neurons. It is not so common anymore to funnel the neurons. Number of neurons chosen should be the same for all layers. This also reduces the number of hyper perameters that need to be tuned. \n",
    "\n",
    "A simple approach is to use the stretch pants method. pick a model with more layers and neurons than you need and use early stopping to prevent overfitting. \n",
    "\n",
    "## Actiation Functions \n",
    "in most casses the ReLu activation function in the hidden layers is faster to compute. Gradient Descent optimization does not get stuck on plateus like it would for large logit values on a sigmoid function. \n",
    "\n",
    "A good choice for classification tasks where the classes are mutually exclusive is to use a softmax activation function for the output layer. when they are not mutually exclusive (or when there are just two classes) it is best to use logistic function. For regression tasics no activation function at all is necessary for the output layer. \n",
    "\n",
    "## Question Answers \n",
    "A Classical Perceptron will converge only if the dataset is linearly separable, and it won't be able to estimate class probabilities. If you alter the perceptrons activation function to a logigistic. It will be able to converge even if the dataset is not linearly seperable. It effectively becomes a logistic classifier. \n",
    "\n",
    "The logistic activation function can always be used with gradient descent optimization as the derivative is always non zero.\n",
    "\n",
    "Backpropagation is a technique used to train ANN. It first computes the gradient of the cost function. and then performs a Gradient descent step using these gradients. To compute the gradients, backpropagation uses reverse-mode autodiff. Reverse-mode autodiff performs a forward pass through a computation graph, computing every node's value for the current training batch, and then it performs a reverse pass, computing all the gradients at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "n_inputs = 28*28 # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") #To feed batches to during training\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "    #Creating layers\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "    # cost funcition\n",
    "    # xentropy is equivalent to applying the softmax activation function\n",
    "    # and then computing cross entropy.\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "\n",
    "    # Training using GradientDescent\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss) #minimizing loss function\n",
    "    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"models/tensorflow/tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = log_dir(\"mnist_dnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
    "\n",
    "\n",
    "m, n = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"models/tensorflow/my_deep_mnist_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_deep_mnist_model\"\n",
    "\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation accuracy: 90.520% \tLoss: 0.35458\n",
      "Epoch: 5 \tValidation accuracy: 95.020% \tLoss: 0.17876\n",
      "Epoch: 10 \tValidation accuracy: 96.380% \tLoss: 0.12997\n",
      "Epoch: 15 \tValidation accuracy: 96.980% \tLoss: 0.10587\n",
      "Epoch: 20 \tValidation accuracy: 97.460% \tLoss: 0.09175\n",
      "Epoch: 25 \tValidation accuracy: 97.560% \tLoss: 0.08408\n",
      "Epoch: 30 \tValidation accuracy: 97.780% \tLoss: 0.07621\n",
      "Epoch: 35 \tValidation accuracy: 97.900% \tLoss: 0.07321\n",
      "Epoch: 40 \tValidation accuracy: 98.020% \tLoss: 0.06968\n",
      "Epoch: 45 \tValidation accuracy: 98.020% \tLoss: 0.06941\n",
      "Epoch: 50 \tValidation accuracy: 98.080% \tLoss: 0.06770\n",
      "Epoch: 55 \tValidation accuracy: 98.220% \tLoss: 0.06633\n",
      "Epoch: 60 \tValidation accuracy: 98.060% \tLoss: 0.06714\n",
      "Epoch: 65 \tValidation accuracy: 98.140% \tLoss: 0.06695\n",
      "Epoch: 70 \tValidation accuracy: 98.180% \tLoss: 0.06636\n",
      "Epoch: 75 \tValidation accuracy: 98.220% \tLoss: 0.06700\n",
      "Epoch: 80 \tValidation accuracy: 98.260% \tLoss: 0.06733\n",
      "Epoch: 85 \tValidation accuracy: 98.220% \tLoss: 0.06790\n",
      "Epoch: 90 \tValidation accuracy: 98.260% \tLoss: 0.06902\n",
      "Epoch: 95 \tValidation accuracy: 98.260% \tLoss: 0.06887\n",
      "Epoch: 100 \tValidation accuracy: 98.220% \tLoss: 0.06961\n",
      "Epoch: 105 \tValidation accuracy: 98.220% \tLoss: 0.07009\n",
      "Epoch: 110 \tValidation accuracy: 98.300% \tLoss: 0.07094\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "        \n",
    "        \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: X_valid, y: y_valid})\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation accuracy: {:.3f}%\".format(accuracy_val * 100),\n",
    "                  \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"Early stopping\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_deep_mnist_model\n",
      "Predicted values:  [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n",
      "Actual Values:  [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path) # or better, use save_path\n",
    "    X_new_scaled = X_test[:20]\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1)\n",
    "    \n",
    "print(\"Predicted values: \", y_pred)\n",
    "print(\"Actual Values: \", y_test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
